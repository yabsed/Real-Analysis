Here are your comprehensive study notes. Based on the provided materials (OSTEP textbook, the 2024 Final Exam, and Lecture Slides), I have synthesized a "Final Exam Cheat Sheet" style document.

It prioritizes the **Fast File System (FFS)** logic, **Concurrency algorithms** (Bakery, Barriers), and **Architecture/OS definitions** found in the exam.

-----

# 4190.307 Operating Systems: Final Exam Study Guide

## Part I. The Fast File System (FFS)

*Goal: Fix the "Old Unix FS" which had poor performance (2% bandwidth utilization) and fragmentation.*

### 1\. The Core Structure: Cylinder Groups

The Old Unix FS treated the disk like RAM (random access). FFS treats the disk like a **disk** (mechanical device with seek/rotation costs).

  * **Cylinder:** Tracks at the same distance from the center across all platter surfaces.
  * **Cylinder Group (CG):** A collection of $N$ consecutive cylinders.
  * **Locality Principle:** Keep related data (inode + data) in the same CG to minimize seek time.

**On-Disk Layout (per Cylinder Group):**

1.  **Super Block (S):** Copy of FS parameters (volume size, block size). Replicated in every CG for reliability (if one corrupts, use another).
2.  **Inode Bitmap (ib):** Tracks allocated/free inodes in this group.
3.  **Data Bitmap (db):** Tracks allocated/free blocks. *Replaces the "Free List" of Old FS to reduce fragmentation.*
4.  **Inodes:** The inode table for this group.
5.  **Data Blocks:** The actual file content.

### 2\. Allocation Policies (The "Heuristics")

FFS uses "Common Sense" heuristics to maintain locality.

**A. Directory Placement**

  * **Goal:** Balance directories across the disk; keep file inodes near their directory.
  * **Policy:** Place a new directory in a CG that has:
    1.  A low number of allocated directories (load balancing).
    2.  A high number of free inodes (room to grow).

**B. File Placement**

  * **Goal:** Reduce seek between inode access and data access.
  * **Policy:** Allocate data blocks in the **same CG** as the file's inode.

**C. Files within a Directory**

  * **Goal:** Support operations like `ls` or `grep *` efficiently.
  * **Policy:** Place all files residing in the same directory into the **same CG**.

**D. The Large-File Exception**

  * **Problem:** A single massive file could fill an entire CG, preventing "related" files (same dir) from entering.
  * **Policy:**
    1.  First 12 blocks (Direct pointers) $\to$ **Same CG** as inode.
    2.  First Indirect Block chunk $\to$ **Different CG**.
    3.  Subsequent chunks $\to$ Spread across other CGs.
  * **Amortization:** Is performance hurt by spreading? No.
      * If the "Chunk Size" is large enough, the **Transfer Time** dominates the **Seek Time**.
      * *Math:* To achieve 50% disk bandwidth, ensure $Time_{transfer} = Time_{positioning}$.

### 3\. FFS Internal Features

**A. Sub-blocks (Internal Fragmentation)**

  * **Problem:** FFS increased block size from 512B to 4KB for throughput. Small files (e.g., 1KB) waste 3KB space (Internal Fragmentation).
  * **Solution:** Divide 4KB block into 512-byte **sub-blocks**.
  * **Mechanism:** Small files use sub-blocks. When file grows to 4KB, copy data to a full block and free sub-blocks. (Optimized by `libc` buffering to avoid excessive copying).

**B. Parameterization (Rotation Latency)**

  * **Problem:** Sequential reads on old disks were too slow. By the time the CPU processed Block 0, Block 1 had already rotated past the head (missed rotation).
  * **Solution:** **Skip Sectors**. Do not allocate physically sequential blocks logically sequentially.
      * Example: Allocating 0, 2, 4... allows the head to catch Block 2 after processing Block 0.
  * **Modern Note:** Modern disks have a **Track Buffer** (internal cache) that reads the whole track, making this manual parameterization obsolete.

-----

## Part II. Concurrency & Algorithms (Code Focus)

*Based on the specific problems in the 2024 Final Exam.*

### 1\. The Bakery Algorithm (Mutual Exclusion)

Ensures thread safety without hardware atomic instructions (software-only solution). Uses the analogy of taking a ticket number.

**Data Structures:**

  * `boolean choosing[N]`: True if thread `i` is currently picking a number.
  * `int number[N]`: The ticket number of thread `i` (0 = not interested).

**The Algorithm (Code Analysis):**

```c
// acquire lock
choosing[me] = TRUE;              // Doorway: I am picking a number
number[me] = max(number[0..N-1]) + 1;
choosing[me] = FALSE;             // Done picking

for (j = 0; j < N; j++) {
    while (choosing[j]);          // WAIT 1: If j is picking, wait for it to finish.
    while (number[j] != 0 &&      // WAIT 2: If j has a ticket...
          (number[j] < number[me] || (number[j] == number[me] && j < me)));
          // ...and j has a smaller ticket (higher priority), wait for j.
}
// Critical Section
```

**Critical Question: Why is `choosing` necessary?**

  * **Scenario without `choosing`:**
    1.  Thread A reads `max` (calculates, say, 5).
    2.  Thread B reads `max` (sees 5).
    3.  Thread A is interrupted *before* writing 6 to `number[A]`. Currently `number[A]` is 0.
    4.  Thread B writes 6 to `number[B]` and enters the check loop.
    5.  B sees `number[A] == 0`. B thinks A is not interested. B enters Critical Section.
    6.  A resumes, writes 6 to `number[A]`.
    7.  A enters loop. A sees `number[B] == 6`. Tie-breaker `(A < B)` might allow A to enter (depending on ID).
    <!-- end list -->
      * **Result:** Violation of Mutual Exclusion.
  * **Starvation:** Does not occur. The FIFO nature (ticket numbers) guarantees fairness.

### 2\. Barriers (Synchronization)

A point where threads must wait until *all* $N$ threads arrive.

**A. Implementation using Semaphores (for 2 threads)**

```c
sema_t s[2]; // Init to 0

void barrier2(int me) { // me is 0 or 1
    int other = 1 - me;
    sema_signal(&s[other]); // Tell the other thread "I arrived"
    sema_wait(&s[me]);      // Wait for the other thread to signal me
}
```

**B. Implementation using Mutex & CondVar (Generalized for N)**

  * **Logic:** Last thread to arrive wakes everyone else up.
  * **Mesa Semantics:** Must use `while` loop and `broadcast`.

<!-- end list -->

```c
mutex_t m;
cond_t cv;
int count = 0;
int N; // Total threads

void barrier(int me) {
    mutex_lock(&m);
    count++;
    if (count == N) {
        cond_broadcast(&cv); // Wake everyone
        count = 0;           // Reset for next phase (reusable barrier)
    } else {
        while (count < N) {  // Wait for stragglers
            cond_wait(&cv, &m);
        }
    }
    mutex_unlock(&m);
}
```

-----

## Part III. Architecture & OS Fundamentals

*Key terminology and concepts appearing in the exam.*

### 1\. Thread State Sharing

When a process creates POSIX threads, what is shared?

  * **Shared (Heap/Global):** Heap memory, Global variables, Open file descriptors, Code segment.
  * **Private (Thread-Local):** **Stack** (local variables, return addresses), **Register values** (PC, SP), Thread Priority, Signal Mask (blocked signals).

### 2\. Context Switch Costs (Ranking)

Order from Cheapest to Most Expensive:

1.  **Branch/Jump:** CPU instruction, very fast.
2.  **Procedure Call:** Pushes PC/Stack frame. Slight memory overhead.
3.  **User-level Thread Switch:** Save/restore registers. No kernel entry needed.
4.  **Kernel-level Thread Switch:** Requires **Trap** to OS, expensive ring transition, TLB flush implications.

### 3\. Hardware Concepts

  * **PIO (Programmed I/O):** CPU manually moves data between device and RAM. High CPU overhead.
  * **DMA (Direct Memory Access):** Device controller moves data to RAM. CPU only handles interrupt at start/end.
  * **Spindle:** The motor shaft that spins the hard drive platters.
  * **Track Buffer:** A small cache on the HDD itself. It reads the *entire track* continuously to service requests for sectors on that track without waiting for rotation.
  * **Read Disturb (SSD):** Reading a specific page in NAND flash many times can cause bit-flips in *neighboring* pages (leakage current). Requires re-writing data periodically.

### 4\. System Calls

  * **fsync(fd):** Forces dirty (buffered) data in memory to be written to the physical disk immediately. Crucial for database durability.

-----

## Part IV. File System Math & Directory Indexing

*Based on the FATty and Indexing questions.*

### 1\. FAT (File Allocation Table) Math

  * **Layout:** Boot | Super | Log | FAT Region | Inode | Data
  * **FAT Entry:** Linked list of blocks. One entry per Data Block.
  * **Calculation:**
      * Disk Size: 4 GiB ($2^{32}$ bytes). Block Size: 4 KiB ($2^{12}$ bytes).
      * Total Blocks: $2^{32} / 2^{12} = 2^{20}$ blocks.
      * Entries needed: $2^{20}$.
      * Size per entry: 24 bits (3 bytes).
      * Total FAT size: $2^{20} \times 3$ bytes = 3 MiB.
      * FAT Blocks needed: 3 MiB / 4 KiB = 768 blocks.

### 2\. Directory Traversal Costs

Operation: `open("/usr/bin/ls")`

  * **Steps (Recursive Traversal):**
    1.  Read Inode of `/` (Root, usually fixed \#0 or \#2).
    2.  Read Data of `/` $\to$ Find "usr" $\to$ Get Inode of `usr`.
    3.  Read Inode of `usr`.
    4.  Read Data of `usr` $\to$ Find "bin" $\to$ Get Inode of `bin`.
    5.  Read Inode of `bin`.
    6.  Read Data of `bin` $\to$ Find "ls" $\to$ Get Inode of `ls`.
    7.  Read Inode of `ls` (to return handle).

### 3\. Full Path Indexing (Optimization)

Instead of recursive lookup, maintain a global B-Tree or Hash Map: `"/usr/bin/ls" -> Inode #10`.

  * **Pros (Time Complexity):** Lookup is $O(1)$ or $O(L)$ (where $L$ is path length), avoiding multiple disk reads for parent directories.
  * **Cons (Space & Maintenance):**
      * **Space:** Storing full strings for every file is huge compared to hierarchical structure.
      * **Renaming:** `mv /usr /data` requires updating **every** entry starting with `/usr/...`. Massive write amplification.
      * **Hard Links:** Difficult to map multiple paths to one inode efficiently.
      * **Symbolic Links:** The index maps Path $\to$ Inode. A symlink is a file containing another path.
          * *Implementation:* Index maps `/mylink` $\to$ Inode X. Inode X contains string `/target`. OS reads `/target` and queries index again.