Here is a comprehensive, high-density study guide organized for rapid absorption. It synthesizes the provided exam materials, textbook content, and lecture slides into a self-contained logic flow, focusing heavily on architecture, algorithms, and system design trade-offs.

---

# **Operating Systems: Final Exam Study Guide**
**Focus:** Concurrency, Flash/SSDs, File System Architecture
**Source Material:** Fall 2025 Course Material & Spring 2024 Exam Analysis

---

## **I. OS & Concurrency Fundamentals (Exam Patterns)**

### **1. Thread & Process Architecture**
* **Shared State (Threads):** Heap memory, Global variables, Open files (file descriptors), Signal handlers, Parent PID.
* **Private State (Threads):** Registers, Stack (local variables, return addresses), Program Counter (PC), Thread ID.
* **Cost Hierarchy (Cheapest to Most Expensive):**
    1.  **Branch/Jump:** CPU instruction level.
    2.  **Procedure Call:** Push stack frame, jump.
    3.  **User-level Thread Switch:** Save/restore user registers; no kernel involvement.
    4.  **Kernel-level Thread Switch:** Trap to kernel, full context switch (tlb flush, cache pollution implications).

### **2. Synchronization Algorithms**
**A. The Bakery Algorithm (Software Mutual Exclusion)**
* **Concept:** Threads "take a number" (like a deli). The thread with the lowest number enters the Critical Section (CS).
* **Crucial State:** `choosing[i]` (bool) and `number[i]` (int).
* **The Logic:**
    1.  Set `choosing[i] = true`.
    2.  `number[i] = max(number[0]...number[n]) + 1`.
    3.  Set `choosing[i] = false`.
    4.  **Wait Loop:** Wait for threads with smaller numbers, or same number but smaller PID.
* **Hardware Optimization:** Using `fetch_and_add` removes the need for the `choosing` array and the loop to calculate max, as the hardware guarantees a unique, atomic ticket number.

**B. Barrier Synchronization**
* **Goal:** $N$ threads must all reach a point before any proceed.
* **Implementation (Semaphore):**
    * Need a counter and a lock (mutex).
    * Last thread arriving triggers a "turnstile" (signals the semaphore) to release everyone.
* **Implementation (Condition Variable):**
    * `wait` loop: `while (count < N) cond_wait(&cv, &lock);`
    * Last thread: `cond_broadcast(&cv);`

---

## **II. Flash Memory Architecture (The "Beast")**

### **1. NAND Flash Physics & Constraints**
* **Cell Types:**
    * **SLC (Single Level):** 1 bit/cell. Fastest, longest life (~100k cycles), expensive.
    * **MLC (Multi):** 2 bits/cell.
    * **TLC (Triple):** 3 bits/cell. Slower, lower endurance (~3k cycles), cheapest.
* **Organization Hierarchy:**
    * **Plane/Bank:** Independent parallel units.
    * **Block (Erase Unit):** 128KB - 2MB. Composed of many pages.
    * **Page (Read/Program Unit):** 4KB - 16KB.
* **The Three Operations:**
    1.  **Read (Page):** Fast ($10-100 \mu s$). Random access is efficient.
    2.  **Program (Page):** Change bits from $1 \to 0$. Medium speed. **Cannot overwrite** (must erase first). Must write pages sequentially (Low $\to$ High) to prevent *Program Disturbance* (bit flips in neighbors).
    3.  **Erase (Block):** Change bits from $0 \to 1$. Slow ($ms$). Bulk operation only.

### **2. Reliability Issues**
* **Wear Out:** Oxide layer degrades after repeated P/E cycles.
* **Disturbance:** Reading/Programming a page stresses neighboring cells, causing bit flips.
* **Retention:** Charge leakage over time (data fades).

---

## **III. Flash Translation Layer (FTL)**

The FTL is the "Magic" software/firmware turning raw Flash into a logical block device (like a generic HDD).

### **1. Core Responsibilities**
* **Logical to Physical Mapping:** Map LBA (Logical Block Address) to PPN (Physical Page Number).
* **Garbage Collection (GC):** Reclaim invalid space.
* **Wear Leveling:** Distribute writes evenly across all physical blocks.

### **2. Log-Structured FTL (The Standard)**
* **Concept:** Never overwrite. Always write new data to the next free page in the current "Log Block".
* **State:** Pages are `Invalid` (old data), `Valid` (current data), or `Erased` (free).
* **Trim Command:** OS informs SSD that specific LBAs are deleted. SSD invalidates pages immediately (helps GC).

---

## **IV. Mapping Schemes (Architecture & Algorithms)**

This is the most critical architectural trade-off in SSD design.

### **1. Page-Level Mapping (High Performance, High RAM Cost)**
* **Mechanism:** Table maps *every* Logical Page to a Physical Page.
* **Memory Cost:** logical\_space / page\_size $\times$ entry\_size.
    * *Example:* 1 TB SSD, 4KB Page, 4B Entry $\to$ **1 GB DRAM** required.
* **Pros:** Flexible placement, high performance.
* **Cons:** Too much DRAM needed for large SSDs.

### **2. Block-Level Mapping (Low Performance, Low RAM Cost)**
* **Mechanism:** Map Logical *Chunk* (size of a block) to Physical *Block*. Offset within block remains constant.
* **Memory Cost:** Reduced by factor of (Pages per Block). Very small.
* **The "Small Write" Problem:**
    * To modify Logical Page 0 in Block A:
        1. Read entire Block A to RAM.
        2. Modify Page 0 in RAM.
        3. Erase Block A (or pick new Block B).
        4. Write all pages back.
    * *Result:* Massive Write Amplification.

### **3. Hybrid Mapping (The Compromise)**
Uses **Block Mapping** for the majority of data, but a small pool of **Log Blocks** uses **Page Mapping**.
* **Log Blocks:** Act as a write buffer. Random writes go here (Page Mapped).
* **Data Blocks:** Long-term storage (Block Mapped).
* **Merge Operations (Moving from Log to Data):**
    1.  **Switch Merge (Best):** Log block contains exact valid pages to replace a Data block. Just update the Block Map pointer. (Cheap).
    2.  **Partial Merge:** Log block has *some* updates. Copy missing valid pages from old Data Block $\to$ Log Block. Switch Log Block to be the new Data Block.
    3.  **Full Merge (Worst):** Multiple Log blocks contain updates for one Data chunk. Allocate new block, gather valid pages from Log blocks and old Data block. High overhead.

---

## **V. Garbage Collection (GC) Algorithm**

GC runs when free blocks are low (Watermarks: High/Low).

**Algorithm:**
1.  **Identify Victim:** Find a block with mostly `Invalid` pages (Greedy policy).
2.  **Read Valid:** Read the remaining `Valid` pages from the victim into memory.
3.  **Write Log:** Append these valid pages to the current open Log Block.
4.  **Update Map:** Update mapping table for moved pages.
5.  **Erase:** Erase the victim block and add to Free List.

**Write Amplification (WA):**
$$WA = \frac{\text{Total Physical Writes}}{\text{Logical Writes Issued by Host}}$$
* Ideal WA = 1.0 (Sequential writes, no GC).
* High WA kills SSD lifespan and performance. Over-provisioning (keeping hidden spare capacity) reduces WA by making GC more efficient.

---

## **VI. File System Calculations (Code & Math)**

Based on the "FATty" and "Fast File System" context.

### **1. FAT (File Allocation Table) Math**
* **Concept:** A linked list stored in a fixed region. One entry per data block.
* **Calculation:**
    * Capacity: $4 \text{ GiB} = 2^{32} \text{ bytes}$.
    * Block Size: $4096 \text{ bytes} = 2^{12} \text{ bytes}$.
    * Total Blocks: $2^{32} / 2^{12} = 2^{20}$ blocks.
    * FAT Entry Size: 24 bits = 3 bytes.
    * Total FAT Size: $2^{20} \times 3 \text{ bytes} = 3 \text{ MB}$.
    * **Blocks needed for FAT:** $3 \text{ MB} / 4 \text{ KB} = 768$ blocks.

### **2. Inode & Directory Traversal Costs**
**Scenario:** `open("/usr/bin/ls", O_RDONLY)`
* **Standard Recursive Traversal:**
    1.  Read Root Inode (fixed #).
    2.  Read Root Data (find "usr" -> get inode #).
    3.  Read "usr" Inode.
    4.  Read "usr" Data (find "bin" -> get inode #).
    5.  Read "bin" Inode.
    6.  Read "bin" Data (find "ls" -> get inode #).
    7.  Read "ls" Inode.
* **Cost Calculation:**
    * If directories are large, reading Directory Data might require multiple block reads.
    * *Worst case:* The entry you need is in the last block of the directory file.

### **3. Full Path Indexing**
* **Concept:** Eliminate recursion. One giant map: `"/usr/bin/ls" -> Inode 10`.
* **Time Complexity:**
    * Traditional: $O(d \times n)$ where $d$=depth, $n$=entries per dir (linear scan).
    * Full Path Index: $O(1)$ (with hash map) or $O(\log N)$ (with B-tree).
* **Space Complexity:** Higher. Stores full string paths which are redundant (`/usr`, `/usr/bin`, `/usr/bin/a`...).
* **Issues:**
    * **Rename:** Renaming `/usr` to `/user` requires updating **every** entry starting with `/usr`. Very expensive.
    * **Hard Links:** Difficult to map multiple paths to one inode efficiently.

---

## **VII. Advanced SSD Concepts**

### **1. NVMe (Non-Volatile Memory Express)**
* **Architecture:** Direct PCIe connection (bypass SATA bottleneck).
* **Queues:** Supports 64K queues, each with 64K commands.
* **Doorbell Mechanism:** Host writes to a memory-mapped register ("Doorbell") to wake up the controller for new IO. Reduces CPU overhead compared to legacy interrupts/polling.

### **2. IOPS & Throughput**
* **Sequential:** SSD $\approx$ 500MB/s - 7GB/s. HDD $\approx$ 200MB/s.
* **Random:** SSD $\approx$ 100k+ IOPS. HDD $\approx$ 100-200 IOPS.
* **Why is Random Write good on SSD?** Because Log-structured FTL turns random logical writes into sequential physical writes.

### **3. Superblock & Boot Block**
* **Boot Block:** First block, code to bootstrap OS.
* **Super Block:** Metadata about the file system (block size, total count, inode table location). If this corrupts, FS is dead.

---

### **Actionable Next Step**
Would you like me to generate **practice calculation problems** specifically for the FAT/Inode sizing or **pseudo-code tracing** for the Bakery/Barrier algorithms to test your readiness?